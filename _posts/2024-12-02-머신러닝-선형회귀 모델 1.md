---
title: "머신러닝 - 선형회귀 모델 1"
categories:
  - 머신러닝
  - 회귀 알고리즘
tags:
  - AI
  - Andrew Ng
  - 지도학습
---

본 글은 Coursera, Andrew Ng 교수님의
Supervised Machine Learning: Regression and Classification 강의를 듣고 이해한 것을 토대로 작성되었음.

파이썬의 numpy 모듈을 사용 할 줄 알고, 간단한 수학적 지식이 있으면<br> 글을 이해하는데 도움이 된다. (사전 지식이 없어도 이해할 수 있다.)

---

글을 시작하기 앞서, 일단 `지도학습`과 `비지도학습`이 무엇인지 알고 넘어가자.

`지도학습`이란:

- 입력 $x$와 출력 $y$를 `학습`시켜, 새로운 $x$가 있을 때의 $y$를 `예측`하는 것이다.
- 학습된 $y$는 `정답`이므로, $x$에서 $y$로 가는 `일련의 패턴`을 가지도록 `지도`하는 것이다.

`비지도학습`이란:

- 입력 $x$와 출력 $y$를 `학습`시켜, `패턴`이나 `구조`를 `추론`하는 것이다.
- 학습된 $y$는 `정답인지 아닌지 모르므로`, 어떠한 `특징`이 있는지 유추하는 것이다.

우리는 이 글에서 `지도학습`의 방법 중 하나인 `회귀 알고리즘`, 그 중에서 `선형회귀 알고리즘`에 대해서 배워볼 것이다.

---

`선형회귀 알고리즘`이란:

- 입력 $x$와 출력 $y$의 `추이`가 `직선의 형태`(=선형)로 나타나는 알고리즘이다.
- `회귀`란 어떠한 값이 특정 추이를 따른다는 `가정`을 의미한다.
  - 가령, 평균 170cm의 키를 가진 가족이라면, 세대를 거듭해도 키의 `평균`은 170cm로 `돌아온다`.

### 용어 정리

본 글에서 사용할 용어는 다음과 같다:

- `Scalar`: 1차원 배열
- `Vector`: n차원 배열
- `Training Set`, `Dataset`: 학습에 필요한 입력, 출력값의 묶음
- $x$, `Feature`: 입력
- $y$, `Target`: 출력
- $\hat{y}$, `y-hat`: $y$의 예측값
- `parameter`: 수정 가능한 변수
- $w$, `weight`: 가중치, parameter
- $b$, `bias`: 편향, parameter
- `Model`: 모델, 알고리즘을 기반으로 만든 함수

다수의 $x$를 가진 `다중 선형회귀`를 배우기 앞서, 먼저 단일 $x$를 가진 `일변량 선형회귀`에 대해 알아보겠다.

---

### 일변량 선형회귀 모델(Univariate Linear Regresssion)

![image](https://github.com/user-attachments/assets/491911b5-9ccf-4fec-9e77-cccc4a52fd5c)
(강의 중 일부를 발췌하여 수정하였음.)

위는 집 크기($x$)와 집 가격($y$)를 `일변량 선형회귀 모델`의 그래프로 표현한 것이다. <br>
만약 3000 $\text{feet}^2$의 집이 매물로 올라왔다면, 집 가격은 몇 달러일까?<br>
선형회귀 모델에 의하면, 추이에 따라 아마 500달러 내외의 값을 가질 것이다.

위 그래프를 함수로 표현해보자.
먼저, `일변량 선형회귀 모델`의 함수는 다음과 같다.<br>
$$\hat{y}=wx+b$$
모델의 $y$는 `예측값`이므로, $\hat{y}$로 표현된다.
그래프에 따르면, $x$가 0일 때 $\hat{y}$도 0이므로, $b$는 0이다. <br>
다음으로, $x$가 600(편의상 가정한 값이다.)일 때, $y$는 100이므로, $w$는 $1/6$이다. <br> 따라서, 완성된 공식은 아래와 같다.
$$\hat{y}=\frac{1}{6}x+0$$

공식을 찾아내기 앞서 우리는 3000 $\text{feet}^2$의 집이 500달러일 것으로 예상하였다. <br> 공식에 대입해보면, 맞게 잘 예상했다는 것을 알 수 있다.

---

### 비용 함수(Cost Function)

`일변량 선형회귀 모델`을 통해 집값이 500달러임을 `예측`할 수 있었다.
그런데 이것이 실제 집값과 일치할까? 우리는 이 `예측`에 대한 `성능`을 `비용 함수`로 표현할 수 있다. 함수부터 먼저 보도록 하자.
$$J(w,b)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(\hat{y}^{(i)}-y^{(i)})^2$$
보기만 해도 어지럼증이 생긴다. 하나하나 파헤쳐보도록 하자.<br>
일단, 이 함수는 `자주 쓰이는 함수` 중 하나인 `MSE`, Mean Squared Error 함수라고 불리는 `평균제곱오차` 함수이다.

- $J(w,b)$: parameter $w$, $b$에 의해 값이 결정되는 `비용함수` $J$
- $\displaystyle\sum_{i=1}^m(\hat{y}^{(i)}-y^{(i)})^2$: $i$가 1부터 $x$의 개수 $m$까지 있을 때, 각 **$i$번째에 해당하는 $\hat{y}$와 $y$의 차이의 제곱**을 $m$번 합한 값. **$i$제곱을 의미하는 것이 아니다.**
- $\frac{1}{2m}$: **평균**제곱오차 함수이므로, $x$의 개수 $m$만큼 나눈다. 이때 $2m$으로 나누는 이유는, 이후 `편미분` 시의 `편의` 때문이므로, `2는 없어도 결과에 영향을 주지 않는다`.

파헤쳐보니 이제 이해가 조금 간다.
완전히 이해해보기 위해, 예제를 통해 응용해보자.
<br>아래는 $\hat{y}=\frac{1}{6}x+0$ 모델을 사용했다고 가정한 집값의 예시이다.

| 예측 집값 $\hat{y}$ | 실제 집값 $y$ |
| :------------------ | ------------- |
| 100                 | 200           |
| 400                 | 300           |
| 500                 | 500           |

$x$는 한 개의 $y$를 가지므로, $x$의 개수 $m$은 3이다.<br>
$i$가 1, 첫번째 예측과 실제의 차이는 -100이다. <br>`MSE` 공식에 따르는 것이므로, 제곱했을 때 10000의 값을 가진다. <br>$i$가 2일때는 100과 10000, $i$가 3일때는 0과 0이다. <br><br>
제곱의 의미를 알겠는가? 제곱을 하지 않으면, -100 + 100 + 0 = 0의 값을 가지게 된다. <br>그러나 $i$가 1과 2일 때 `예측값은 실제와 다르다`. `오차`함수의 값이 0일 수 없는 것이다.<br> 이러한 경우를 방지하기 위해 차이에서 제곱을 한다.<br><br>여기서 드는 의문이 한 가지 더 있다. <br>그렇다면, 제곱이 아닌 `절대값`을 사용하면 되는 것 아닌가?<br>맞다. `MAE`, Mean Absolute Error 함수는 제곱이 아닌 `절대값`을 사용한다. <br> 그런데 왜 `MSE`를 자주 사용할까? 그 이유는 아래와 같다. <br>

1. 오차에 대한 `패널티`를 강하게 줄 수 있다.
2. `미분`이 가능해진다.<br>

1번에 따르면, `이상치`(outlier)에 취약하다는 생각이 든다. 맞는 말이다. <br> 중요한 것은, `데이터와 모델에 맞는 비용 함수`를 선택하는 것이다.<br> 2번과 `MSE` 공식의 $\frac{1}{2m}$에 대해서는,<br> 다음 글에서 작성할 `경사하강법`에서 소개할 예정이므로 잠시 잊도록 하자.

다시 본론으로 넘어와서, 계속 `MSE` 함수로 진행하겠다. <br>
$$J(w,b)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(\hat{y}^{(i)}-y^{(i)})^2$$
각 $i$번째 차이의 제곱을 더하면, 10000 + 10000 + 0 = 20000의 값을 얻는다. <br>
이를 $\frac{1}{2m}$로 나누면, $m$이 3이므로, 20000 / 6 = 약 3333.3의 결과를 가진다.
<br> 앞선 설명해서 유추할 수 있듯, `비용함수가 0에 가까울수록 예측값과 결과값의 차이가 없다`. <br><br> $x$와 $y$를 수정하는 것은 데이터 조작에 해당하는 일이므로, 올바른 모델을 만들 수 없다. <br> 따라서, 비용함수에서 수정 가능한 변수, 즉 `parameter`는 `가중치` $w$와 `편향` $b$밖에 없다. <br> $\hat{y}$이 $wx+b$의 값을 가지기 때문이다. 그래서, 비용함수가 $J(w,b)$의 모양을 하는 것이다. <br><br> 그렇다면 우리는 마지막 결론에 다다르게 된다. <br>
$J(w,b)$의 값이 0이 되게 하는 최적의 $w$,$b$는 어떻게 구하는가? <br> 이는 잠깐 언급되었던 `경사하강법`으로 구할 수 있다. <br> 다음 글에서 알아보도록 하자.
<br>

---

유용균, 2024-12-06 <br>
Coursera, Andrew Ng 교수님께 감사드립니다.
